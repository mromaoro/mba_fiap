{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mromaoro/mba_fiap/blob/main/reforcement_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owx1tRE_hv7X"
      },
      "source": [
        "#Desafio Reforcement Learning\n",
        "##Contexto:\n",
        "Imagine que você está trabalhando em uma empresa de logística que opera uma frota de robôs de entrega autônomos. Sua tarefa é implementar um algoritmo de aprendizado por reforço para otimizar a entrega de pacotes em um ambiente simulado. Os robôs têm a capacidade de se mover em um ambiente em grade 2D e devem aprender a tomar decisões sobre para onde se mover para entregar pacotes de forma eficiente.\n",
        "##Objetivo:\n",
        "O objetivo do desafio é implementar um agente de aprendizado por reforço usando ténicas como por exemplo Q-Table, equação de Bellman, ε-greedy e etc, para maximizar o retorno cumulativo ao entregar pacotes no menor tempo possível, considerando um \"living penalty\" para incentivar o agente a ser eficiente.\n",
        "##Tarefas:\n",
        "  * Modelagem do ambiente: Crie um ambiente 2D simulado, onde o agente pode se mover em um grid. Considere que o ambiente tem obstáculos e pontos de entrega de pacotes.\n",
        "\n",
        "* Definição do MDP: Modele o problema como um MDP, definindo os estados, as ações, as recompensas, a função de transição e o fator de desconto.\n",
        "* Implementação da Q-Table: Crie uma Q-Table para representar o valor estimado de cada par (estado, ação).\n",
        "\n",
        "* Implementação do agente: Desenvolva um agente de aprendizado por reforço que utiliza a equação de Bellman para atualizar a Q-Table com base nas recompensas recebidas ao longo do tempo.\n",
        "* Living Penalty: Introduza um \"living penalty\" para penalizar o agente por gastar muito tempo no ambiente. Isso deve incentivar o agente a encontrar a rota mais eficiente para entregar os pacotes.\n",
        "* Treinamento e avaliação: Treine o agente usando um algoritmo de aprendizado por reforço, como o Q-Learning, e avalie seu desempenho em termos de eficiência na entrega de pacotes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8EUiIbabfK6"
      },
      "source": [
        "#Iniciando meu projeto\n",
        "\n",
        "1. Definir o Ambiente\n",
        "* Função __init__:\n",
        ">Inicializa o ambiente, define o espaço de estados, o espaço de ações, e quaisquer parâmetros relevantes do ambiente.\n",
        "\n",
        "\n",
        "* Função reset:\n",
        ">Reinicia o ambiente para um estado inicial e retorna esse estado. É útil no início de cada episódio.\n",
        "\n",
        "* Função step:\n",
        "> Executa uma ação dentro do ambiente, retorna o novo estado, a recompensa obtida, e um sinalizador indicando se o episódio terminou (por exemplo, done).\n",
        "\n",
        "* Função render (opcional):\n",
        "> Fornece uma representação visual do estado do ambiente. Útil para depuração e compreensão do comportamento do agente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "isocdTTuRB_x",
        "outputId": "a8ccde52-d62d-4c17-b650-20fd127101ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "def env(start,goal):\n",
        "  env_array = np.array([\n",
        "      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "      [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
        "      [1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1],\n",
        "      [1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1],\n",
        "      [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1],\n",
        "      [1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1],\n",
        "      [1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1],\n",
        "      [1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1],\n",
        "      [1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1],\n",
        "      [1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1],\n",
        "      [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1],\n",
        "      [1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1],\n",
        "      [1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1],\n",
        "      [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
        "      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "\n",
        "  ])\n",
        "  env_array[start[0], start[1]] = 2\n",
        "  env_array[goal[0], goal[1]] = 9\n",
        "  return env_array\n",
        "\n",
        "env_viz = env([1,1],[11,11])\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(env_viz, cmap='BrBG', interpolation='none')\n",
        "plt.show()\n",
        "\n",
        "ob = [\n",
        "  [2,2],[3,2],[5,2],[8,2],[10,2],[12,2],\n",
        "  [2,3],[4,3],[6,3],[9,3],[11,3],[12,3],\n",
        "  [2,5],[3,5],[5,5],[6,5],[7,5],[8,5],[9,5],[11,5],[12,5],\n",
        "  [7,6],[7,7],\n",
        "  [2,8],[3,8],[5,8],[6,8],[7,8],[8,8],[9,8],[11,8],[12,8],\n",
        "  [2,9],[12,9],\n",
        "  [2,10],[3,10],[4,10],[10,10],[11,10],[12,10],\n",
        "  [2,11],[12,11],\n",
        "  [2,12],[3,12],[4,12],[5,12],[6,12],[7,12],[8,12],[9,12],[10,12],[11,12],[12,12],\n",
        "  [0,0],[1,0],[2,0],[3,0],[4,0],[5,0],[6,0],[7,0],[8,0],[9,0],[10,0],[11,0],[12,0],[13,0],[14,0],\n",
        "  [0,0],[0,1],[0,2],[0,3],[0,4],[0,5],[0,6],[0,7],[0,8],[0,9],[0,10],[0,11],[0,12],[0,13],[0,14],\n",
        "  [14,0],[14,1],[14,2],[14,3],[14,4],[14,5],[14,6],[14,7],[14,8],[14,9],[14,10],[14,11],[14,12],[14,13],[14,14],\n",
        "  [0,14],[1,14],[2,14],[3,14],[4,14],[5,14],[6,14],[7,14],[8,14],[9,14],[10,14],[11,14],[12,14],[13,14],[14,14]\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IMaQuASUFizs"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "\n",
        "\n",
        "class GridWorldEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Ambiente personalizado de grid 15x15 para Reinforcement Learning.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(GridWorldEnv, self).__init__()\n",
        "        self.grid_size = 15\n",
        "        self.action_space = spaces.Discrete(4)  # 0: para cima, 1: para baixo, 2: esquerda, 3: direita\n",
        "        self.observation_space = spaces.Discrete(self.grid_size * self.grid_size)\n",
        "\n",
        "        # Definir o estado inicial e o objetivo\n",
        "        self.agent_pos = [1, 1]\n",
        "        self.goal_pos = [11, 11]\n",
        "\n",
        "        # Criar obstáculos\n",
        "        self.obstacles = ob\n",
        "\n",
        "        # Certificar de que o objetivo e a posição inicial não são obstáculos\n",
        "        self.obstacles = [obs for obs in self.obstacles if obs not in [self.agent_pos, self.goal_pos]]\n",
        "\n",
        "    def reset(self):\n",
        "        # Resetar a posição do agente para o início\n",
        "        self.agent_pos = [1, 1]\n",
        "        return self._get_obs()\n",
        "\n",
        "    def _get_obs(self):\n",
        "        # Obter a observação (estado) atual\n",
        "        obs = np.zeros((self.grid_size, self.grid_size))\n",
        "        obs[tuple(self.agent_pos)] = 1\n",
        "        obs[tuple(self.goal_pos)] = 2\n",
        "        for obs_pos in self.obstacles:\n",
        "            obs[tuple(obs_pos)] = -1\n",
        "        return obs.flatten()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        prev_distance = abs(self.agent_pos[0] - self.goal_pos[0]) + abs(self.agent_pos[1] - self.goal_pos[1])\n",
        "\n",
        "        # Atualizar a posição do agente com base na ação\n",
        "        if action == 0 and self.agent_pos[0] > 0:  # Para cima\n",
        "            self.agent_pos[0] -= 1\n",
        "        elif action == 1 and self.agent_pos[0] < self.grid_size - 1:  # Para baixo\n",
        "            self.agent_pos[0] += 1\n",
        "        elif action == 2 and self.agent_pos[1] > 0:  # Esquerda\n",
        "            self.agent_pos[1] -= 1\n",
        "        elif action == 3 and self.agent_pos[1] < self.grid_size - 1:  # Direita\n",
        "            self.agent_pos[1] += 1\n",
        "\n",
        "        new_distance = abs(self.agent_pos[0] - self.goal_pos[0]) + abs(self.agent_pos[1] - self.goal_pos[1])\n",
        "\n",
        "\n",
        "        # Verificar se chegou ao objetivo\n",
        "        done = self.agent_pos == self.goal_pos\n",
        "\n",
        "        # Calcular a recompensa\n",
        "        if done:\n",
        "            reward = 10  # Recompensa grande por atingir o objetivo\n",
        "        elif self.agent_pos in self.obstacles:\n",
        "            reward = -10  # Penalidade grande por atingir um obstáculo\n",
        "            self.reset()\n",
        "        else:\n",
        "            # Recompensa/penalidade baseada na mudança de distância para o objetivo\n",
        "            if new_distance < prev_distance:\n",
        "                reward = 1  # Recompensa por se aproximar do objetivo\n",
        "            else:\n",
        "                reward = -2  # Penalidade por se afastar do objetivo ou ficar parado\n",
        "\n",
        "\n",
        "        # return self._get_obs(), reward, done, {}\n",
        "        return self.agent_pos, reward, done, {}\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        # Configurar o tamanho da figura\n",
        "        plt.figure(figsize=(5, 5))\n",
        "        # Desenhar o grid\n",
        "        for i in range(self.grid_size):\n",
        "            for j in range(self.grid_size):\n",
        "                # Desenhar o agente\n",
        "                if [i, j] == self.agent_pos:\n",
        "                    plt.text(j, i, 'A', ha='center', va='center', color='blue', fontsize=18)\n",
        "                # Desenhar o objetivo\n",
        "                elif [i, j] == self.goal_pos:\n",
        "                    plt.text(j, i, 'G', ha='center', va='center', color='green', fontsize=18)\n",
        "                # Desenhar os obstáculos\n",
        "                elif [i, j] in self.obstacles:\n",
        "                    plt.fill_between([j-0.5, j+0.5], [i-0.5], [i+0.5], color='black')\n",
        "\n",
        "        # Definir os limites e ocultar os eixos\n",
        "        plt.xlim(-0.5, self.grid_size - 0.5)\n",
        "        plt.ylim(-0.5, self.grid_size - 0.5)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "\n",
        "        # Desenhar as linhas do grid\n",
        "        for i in range(self.grid_size + 1):\n",
        "            plt.plot([-0.5, self.grid_size - 0.5], [i - 0.5, i - 0.5], color='gray')\n",
        "            plt.plot([i - 0.5, i - 0.5], [-0.5, self.grid_size - 0.5], color='gray')\n",
        "\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "u3EYasr4GHbg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "657bd628-70a2-4a36-eac2-1346c8408a8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "class Agent:\n",
        "  def __init__(self, action_space, state_space, learning_rate, discount_factor, epsilon):\n",
        "    self.action_space = action_space\n",
        "    self.state_space = state_space\n",
        "    self.learning_rate = learning_rate\n",
        "    self.discount_factor = discount_factor\n",
        "    self.epsilon = epsilon\n",
        "    self.q_table = np.zeros((int(state_space * state_space), int(action_space.n))) # criação da Q-table\n",
        "    self.grid_size = grid_size\n",
        "\n",
        "  def choose_action(self, observation, test_mode=False):\n",
        "    observation_unidim = observation[0] * self.grid_size + observation[1]\n",
        "    if test_mode:\n",
        "      return np.argmax(self.q_table[observation_unidim])  # A função np.argmax do NumPy encontra o índice da maior entrada em um array.\n",
        "    else:\n",
        "      if np.random.random() < self.epsilon:  # Exploração\n",
        "          return np.random.choice(self.action_space.n)\n",
        "      else:  # Exploração\n",
        "          return np.argmax(self.q_table[observation_unidim])  # A função np.argmax do NumPy encontra o índice da maior entrada em um array. Quando aplicada à linha selecionada da tabela Q, ela retorna a ação com o maior Q-valor, ou seja, a ação que, de acordo com o conhecimento atual do agente, leva ao melhor resultado esperado a longo prazo.\n",
        "\n",
        "  def update_epsilon(self, min_epsilon, decay_rate):\n",
        "    if self.epsilon > min_epsilon:\n",
        "        self.epsilon *= decay_rate\n",
        "\n",
        "  def learn(self, state, action, reward, next_state, done):\n",
        "    # state_unidim = state[0] * self.grid_size + state[1]\n",
        "    # next_state_unidim = next_state[0] * self.grid_size + next_state[1]\n",
        "    old_value = self.q_table[state, action]\n",
        "    next_max = np.max(self.q_table[next_state])\n",
        "\n",
        "    # Fórmula de atualização Q-learning\n",
        "    new_value = (1 - self.learning_rate) * old_value + self.learning_rate * (reward + self.discount_factor * next_max)\n",
        "    self.q_table[state,action] = new_value\n",
        "\n",
        "  def save_q_table(self, filename):\n",
        "      np.save(filename, self.q_table)\n",
        "\n",
        "  def load_q_table(self, filename):\n",
        "      if os.path.exists(filename):\n",
        "          self.q_table = np.load(filename)\n",
        "      else:\n",
        "          print(\"Q-Table file not found, starting with a new Q-Table.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQ4cTba6X6FD"
      },
      "source": [
        "##3. Loop Principal de Treinamento\n",
        "Loop de Episódios:\n",
        "\n",
        "* Executar múltiplos episódios, onde em cada episódio o agente interage com o ambiente.\n",
        "Dentro de Cada Episódio:\n",
        "\n",
        "* Resetar o ambiente.\n",
        "\n",
        "* Executar ações até o episódio terminar (sinalizado pelo ambiente).\n",
        "\n",
        "* Em cada passo, usar a função step do ambiente e a função learn do agente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnE1sX3lV21A"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# Instanciar o ambiente\n",
        "env = GridWorldEnv()\n",
        "\n",
        "# Parâmetros para o agente\n",
        "learning_rate = 0.5\n",
        "discount_factor = 0.9\n",
        "epsilon = 0.1  # Probabilidade de explorar\n",
        "grid_size = 15  # Tamanho do grid do ambiente GridWorldEnv\n",
        "\n",
        "# Instanciar o agente\n",
        "agent = Agent(\n",
        "    action_space= env.action_space,\n",
        "    learning_rate= learning_rate,\n",
        "    discount_factor= discount_factor,\n",
        "    epsilon= epsilon,\n",
        "    state_space= grid_size\n",
        ")\n",
        "\n",
        "num_episodes = 1 # Número total de episódios para treinamento\n",
        "min_epsilon = 0.01  # Valor mínimo para epsilon\n",
        "decay_rate = 0.99   # Taxa de decaimento de epsilon em 1% a cada episódio\n",
        "total_rewards = []\n",
        "\n",
        "\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = env.agent_pos  # Inicializa o estado do ambiente\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    total_states = []\n",
        "    save_path = '/content/drive/MyDrive/Fiap individual/FIAP - material/Reforcement learning/rl_projeto_integrador/q_table.npy'\n",
        "    agent.load_q_table(save_path)\n",
        "\n",
        "    while not done:\n",
        "\n",
        "        # # Converter o estado para um índice unidimensional\n",
        "        state_index = state[0] * grid_size + state[1]\n",
        "\n",
        "        action = agent.choose_action(state, test_mode=True)  # Agente escolhe uma ação\n",
        "        next_state, reward, done, info = env.step(action)  # Ambiente responde com novo estado e recompensa\n",
        "\n",
        "        # Converter o estado para um índice unidimensional\n",
        "        next_state_index = next_state[0] * int(grid_size) + next_state[1]\n",
        "\n",
        "        agent.learn(state_index, action, reward, next_state_index, done)  # Agente aprende com a experiência\n",
        "        state = next_state  # Atualiza o estado atual para o próximo loop\n",
        "        total_reward += reward\n",
        "\n",
        "        total_states.append(state)\n",
        "        total_rewards.append(total_reward)\n",
        "\n",
        "        qtd_movimentos = len(total_states)\n",
        "\n",
        "        # if episode % 100 == 0:\n",
        "        print(f'Episode: {episode}, Total Reward: {total_reward}, state: {state}, action: {action}, next_state: {next_state}, done: {done}, info: {info}')\n",
        "        env.render()\n",
        "\n",
        "    agent.update_epsilon(min_epsilon, decay_rate)  # Atualiza epsilon\n",
        "\n",
        "    print(f'Episode: {episode}, Total Reward: {total_reward}, Total Movimentos: {qtd_movimentos}, done: {done}')\n",
        "\n",
        "    # Salvar a Q-Table após cada episódio\n",
        "    agent.save_q_table(save_path)\n",
        "\n",
        "    #   # Plotar as recompensas totais por episódio\n",
        "    # plt.plot(total_rewards)\n",
        "    # plt.title(\"Recompensa Total por Episódio\")\n",
        "    # plt.xlabel(\"Episódio\")\n",
        "    # plt.ylabel(\"Recompensa Total\")\n",
        "    # plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "OG8KFECJc1oP",
        "outputId": "85dae99c-7850-42c2-bfbf-1da6a84b028a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -1. -1.  0. -1. -1.  0. -1.\n",
            "  0.  0. -1. -1. -1. -1. -1.  0. -1. -1.  0. -1.  0.  0. -1.  0.  0. -1.\n",
            "  0. -1.  0. -1.  0. -1. -1.  0.  0. -1.  0.  0.  0.  0.  0.  0. -1.  0.\n",
            " -1.  0. -1. -1.  0. -1.  0.  0. -1.  0.  0. -1.  0.  0.  0. -1.  0. -1.\n",
            " -1.  0.  0. -1.  0. -1.  0.  0. -1.  0.  0.  0. -1.  0. -1. -1.  0.  0.\n",
            "  0.  0. -1. -1. -1. -1.  0.  0.  0. -1.  0. -1. -1.  0. -1.  0.  0. -1.\n",
            "  0.  0. -1.  0.  0.  0. -1.  0. -1. -1.  0.  0. -1.  0. -1.  0.  0. -1.\n",
            "  0.  0.  0. -1.  0. -1. -1.  0. -1.  0.  0.  0.  0.  0.  0.  0. -1.  0.\n",
            " -1.  0. -1. -1.  0.  0. -1.  0. -1.  0.  0. -1.  0. -1.  2. -1.  0. -1.\n",
            " -1.  0. -1. -1.  0. -1.  0.  0. -1. -1. -1. -1. -1.  0. -1. -1.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -1. -1. -1. -1. -1. -1. -1.\n",
            " -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-41d4aa7a86dc>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Escolhe a ação no modo de teste\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-59-b9c3cf2b6591>\u001b[0m in \u001b[0;36mchoose_action\u001b[0;34m(self, observation, test_mode)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mobservation_unidim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtest_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobservation_unidim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# A função np.argmax do NumPy encontra o índice da maior entrada em um array.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Exploração\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
          ]
        }
      ],
      "source": [
        "# Supondo que o resto da configuração do ambiente e do agente permaneça o mesmo...\n",
        "\n",
        "num_test_episodes = 10  # Número total de episódios para teste\n",
        "total_rewards = []\n",
        "\n",
        "for episode in range(num_test_episodes):\n",
        "    state = env.reset()  # Reinicia o ambiente para o estado inicial\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    # Carrega a Q-Table uma vez no início do episódio\n",
        "    agent.load_q_table('q_table.npy')\n",
        "\n",
        "    while not done:\n",
        "\n",
        "        print(state)\n",
        "\n",
        "        # Escolhe a ação no modo de teste\n",
        "        action = agent.choose_action(state, test_mode=True)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "    total_rewards.append(total_reward)\n",
        "    print(f'Episode: {episode}, Total Reward: {total_reward}')\n",
        "    env.render()\n",
        "\n",
        "# Aqui você pode adicionar código para plotar ou analisar as recompensas totais\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqYkK2qIYkZq",
        "outputId": "ba47dbdc-e205-4ac1-cbff-a8794155db57"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0.        ,  0.        ,  0.        , -4.67375   ],\n",
              "       [-4.67375   ,  0.725     , -5.        , -8.4125    ],\n",
              "       [-9.4081543 ,  1.2125    , -8.260625  , -7.5       ],\n",
              "       [ 0.        ,  0.75      , -5.        ,  0.        ],\n",
              "       [-5.        ,  0.725     , -4.775     ,  0.        ],\n",
              "       [ 0.        ,  0.5       , -4.67375   ,  0.        ],\n",
              "       [ 0.        ,  0.        , -4.775     , -5.        ],\n",
              "       [-8.4125    ,  0.5       , -5.        , -5.        ],\n",
              "       [-5.        ,  0.        , -4.775     ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [-5.        ,  0.5       ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.5       ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [-5.        , -5.        ,  0.        , -5.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  1.25625   ],\n",
              "       [-4.67375   ,  0.875     , -4.623125  ,  1.0875    ],\n",
              "       [-8.925     , -5.        , -0.9375    ,  0.875     ],\n",
              "       [ 0.        , -7.5       , -1.1875    ,  0.        ],\n",
              "       [-5.        ,  0.        ,  0.        ,  0.75      ],\n",
              "       [-5.        ,  0.        , -1.275     ,  0.5       ],\n",
              "       [-5.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [-4.775     ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.5       ],\n",
              "       [-5.        ,  0.        , -1.        , -1.        ],\n",
              "       [-5.        , -5.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        , -5.        ],\n",
              "       [-5.        , -7.05      ,  0.        , -5.        ],\n",
              "       [-5.        , -5.        ,  0.        ,  0.        ],\n",
              "       [-0.775     ,  0.725     , -5.        ,  0.        ],\n",
              "       [-1.1625    , -7.5       ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  1.0952375 , -5.        ,  0.        ],\n",
              "       [ 0.        ,  1.421625  ,  0.        ,  0.        ],\n",
              "       [-1.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        , -4.3784375 ,  0.        ,  0.        ],\n",
              "       [-1.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        , -7.5       ,  0.        , -1.        ],\n",
              "       [-1.        ,  1.0875    , -5.        , -7.275     ],\n",
              "       [-5.        , -7.275     ,  1.20125   , -7.275     ],\n",
              "       [-5.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.75      ,  0.        , -7.1625    ],\n",
              "       [-7.5       ,  0.5       , -1.        ,  1.0875    ],\n",
              "       [ 0.        , -7.06125   ,  0.        ,  1.32275   ],\n",
              "       [-1.0735125 ,  2.40743809, -1.275     , -7.5       ],\n",
              "       [-5.        ,  0.        ,  0.        ,  0.5       ],\n",
              "       [ 0.        ,  0.5       ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  1.0875    ,  0.        ,  0.        ],\n",
              "       [-5.        ,  0.        , -0.775     ,  1.38125   ],\n",
              "       [-5.        ,  0.75      , -4.3784375 ,  0.        ],\n",
              "       [-5.        , -4.775     ,  0.        , -1.        ],\n",
              "       [-1.17375   ,  0.75      ,  0.        , -6.841875  ],\n",
              "       [-7.5       , -7.275     ,  0.975     , -7.5       ],\n",
              "       [ 0.        , -7.5       , -9.6875    ,  0.        ],\n",
              "       [-1.05      ,  1.1       , -5.        ,  0.        ],\n",
              "       [-7.1625    ,  0.        , -0.775     ,  0.        ],\n",
              "       [ 0.        ,  0.5       , -1.5       ,  2.18037539],\n",
              "       [-0.24397465,  2.08694531, -8.3506875 ,  0.        ],\n",
              "       [ 0.        ,  0.        , -0.6625    ,  0.        ],\n",
              "       [ 0.        ,  0.5       ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.5       ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.8375    ],\n",
              "       [-1.        ,  0.75      , -1.        , -6.34364063],\n",
              "       [-8.148125  ,  2.21384375,  0.        ,  0.        ],\n",
              "       [-0.775     ,  1.4403125 , -7.62753125, -7.5       ],\n",
              "       [-5.        , -7.05      ,  1.55709375, -1.5       ],\n",
              "       [-0.775     ,  0.5       , -9.18      , -4.775     ],\n",
              "       [-7.5       , -4.56125   ,  1.1       ,  0.        ],\n",
              "       [-5.        , -5.        ,  0.        ,  0.75      ],\n",
              "       [-1.3       ,  0.975     , -4.775     ,  0.        ],\n",
              "       [ 0.        ,  0.5       ,  0.        ,  0.        ],\n",
              "       [-4.775     , -4.510625  ,  0.        ,  0.725     ],\n",
              "       [-0.39226289,  1.58046875,  0.        , -7.5       ],\n",
              "       [-1.        ,  0.        , -0.775     ,  1.25625   ],\n",
              "       [ 0.        ,  0.5       , -4.623125  ,  0.75      ],\n",
              "       [-1.        ,  0.5       , -0.775     , -5.        ],\n",
              "       [ 0.        , -5.        ,  0.        ,  0.5       ],\n",
              "       [-1.3       ,  0.        ,  0.        ,  0.975     ],\n",
              "       [-7.59364063,  2.10090312, -0.56125   ,  0.75      ],\n",
              "       [-1.24290625,  0.8375    , -0.6581875 , -8.671875  ],\n",
              "       [ 0.        , -7.91140312,  1.345625  , -1.2353125 ],\n",
              "       [-1.5       ,  1.14375   , -8.01171875, -7.9315625 ],\n",
              "       [-8.24375   , -7.8149668 ,  1.2125    , -8.19875   ],\n",
              "       [-4.775     , -5.        , -5.        ,  1.408125  ],\n",
              "       [-0.775     ,  0.975     , -7.0775625 ,  0.75      ],\n",
              "       [-5.        ,  0.        , -0.56125   , -6.7659375 ],\n",
              "       [-1.5       ,  1.708125  , -1.        ,  0.        ],\n",
              "       [-0.65879688,  1.30125   , -6.9290625 ,  0.        ],\n",
              "       [-5.        ,  0.        , -0.67375   ,  0.        ],\n",
              "       [ 0.        , -7.275     , -5.        ,  0.        ],\n",
              "       [-1.        ,  0.        , -1.5       , -5.        ],\n",
              "       [-5.        , -5.        , -1.        ,  0.        ],\n",
              "       [-0.775     ,  0.        ,  0.        ,  0.        ],\n",
              "       [-0.844375  ,  1.57701563, -1.        ,  1.7575625 ],\n",
              "       [-1.        ,  1.929375  , -0.775     , -7.77471875],\n",
              "       [-8.01171875, -7.65439063,  2.3530625 ,  0.        ],\n",
              "       [-0.97796875,  1.9671875 ,  0.        , -4.37664453],\n",
              "       [-8.838125  , -7.79995605,  2.07785156, -5.        ],\n",
              "       [-5.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [-0.94875   ,  0.        ,  0.        ,  1.31375   ],\n",
              "       [ 0.        , -7.1625    , -0.6625    ,  1.2125    ],\n",
              "       [-6.684375  ,  0.875     , -0.94875   ,  1.74875   ],\n",
              "       [-1.42375   ,  1.91955469, -0.66670313,  0.        ],\n",
              "       [-5.        , -8.3       ,  0.        , -4.6625    ],\n",
              "       [ 0.        ,  1.1       , -7.5       ,  0.        ],\n",
              "       [-1.5       ,  0.        ,  0.        ,  0.        ],\n",
              "       [-5.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.5       ,  0.        ,  0.        ],\n",
              "       [-0.775     ,  0.        , -1.        ,  1.8378125 ],\n",
              "       [-0.64678125,  1.275     , -0.78859375, -7.81371094],\n",
              "       [-6.12212812, -8.94823437,  2.01640625, -0.69003906],\n",
              "       [-0.59751953,  1.728125  , -6.36895312, -8.22905762],\n",
              "       [-9.921875  , -8.19951172,  2.26865234, -6.00120117],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [-1.05      ,  0.        ,  0.        ,  0.        ],\n",
              "       [-1.        ,  0.75      , -1.        ,  0.        ],\n",
              "       [-0.505     ,  0.        , -4.6625    ,  1.26875   ],\n",
              "       [-0.84913359,  1.3715625 , -0.8475    , -8.784375  ],\n",
              "       [-8.75      , -7.010625  , -1.35625   ,  1.1       ],\n",
              "       [-7.1625    ,  0.5       , -7.1625    ,  1.0875    ],\n",
              "       [-7.5       ,  0.875     ,  0.        , -7.5       ],\n",
              "       [ 0.        , -8.525     , -0.99375   ,  0.        ],\n",
              "       [ 0.        ,  0.        , -5.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.93875   ],\n",
              "       [-0.775     ,  0.975     , -1.        , -8.63140625],\n",
              "       [-8.34155859, -8.93257812,  1.408125  , -0.72663086],\n",
              "       [-0.66375   ,  2.24298828, -8.57860156, -8.19055391],\n",
              "       [-8.31581543, -8.75      ,  2.50500195, -7.47136719],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.725     ],\n",
              "       [-1.        ,  0.        , -5.        ,  0.5       ],\n",
              "       [-4.775     ,  0.        , -1.275     ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [-1.1625    ,  1.528125  ,  0.        , -4.67375   ],\n",
              "       [-7.1625    ,  0.725     , -0.8275    ,  0.        ],\n",
              "       [-0.775     ,  1.0875    , -5.        ,  0.        ],\n",
              "       [-1.3       ,  0.5       , -0.67375   ,  0.        ],\n",
              "       [-7.5       ,  0.5       , -1.        ,  0.        ],\n",
              "       [ 0.        ,  0.5       ,  0.        ,  1.126875  ],\n",
              "       [ 0.        , -6.66875   , -1.275     ,  2.325     ],\n",
              "       [-0.841875  ,  5.7665625 , -1.        , -8.62125   ],\n",
              "       [-8.73701562, -6.33473438,  2.534375  , -1.045     ],\n",
              "       [-0.58801172,  1.75859375, -7.951875  , -8.87007031],\n",
              "       [-8.41054687, -8.204375  ,  1.88960938, -6.81459375],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        , -5.        ,  0.        ,  0.8375    ],\n",
              "       [-0.8784375 ,  0.75      , -1.123125  ,  1.615625  ],\n",
              "       [-7.010625  , -9.375     , -0.7796875 ,  0.75      ],\n",
              "       [-1.17375   ,  0.975     , -1.05      ,  0.5       ],\n",
              "       [-1.        ,  0.5       , -0.775     ,  0.        ],\n",
              "       [-4.775     , -8.4125    ,  0.        ,  0.5       ],\n",
              "       [-0.775     ,  0.        , -1.5       , -5.475     ],\n",
              "       [-1.3096875 , -5.        , -1.5       ,  3.        ],\n",
              "       [-0.1875    , 11.4625    , -6.15      , -6.91129688],\n",
              "       [-8.17330078, -4.098125  ,  5.7978125 , -1.228125  ],\n",
              "       [-0.80546875,  0.51875   , -5.77553906, -8.55375   ],\n",
              "       [-8.45939063, -7.275     ,  1.528125  , -6.6815625 ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        , -5.        ,  0.        ,  0.        ],\n",
              "       [-0.776875  , -1.        ,  0.        , -5.        ],\n",
              "       [-1.24375   , -7.5       , -1.5       ,  0.        ],\n",
              "       [-1.275     , -0.775     , -5.        ,  0.75      ],\n",
              "       [-1.        , -1.        , -1.        , -4.775     ],\n",
              "       [-1.5       , -5.        , -1.        ,  0.75      ],\n",
              "       [-1.        , -8.13125   , -7.05      ,  0.        ],\n",
              "       [-7.1625    , -7.5       ,  0.        , 11.4625    ],\n",
              "       [ 1.625     , -3.        , -3.3703125 ,  0.        ],\n",
              "       [ 0.        , -5.        , 11.46245469, -2.30546875],\n",
              "       [-1.234375  , -1.275     , -2.8971875 , -4.775     ],\n",
              "       [-7.8753125 ,  0.        ,  0.5       , -4.775     ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        , -1.5       ,  0.        ,  0.5       ],\n",
              "       [ 0.5       , -1.5       ,  0.        ,  0.        ],\n",
              "       [-5.        ,  0.        , -0.775     ,  0.        ],\n",
              "       [ 1.1       , -0.8475    ,  0.        ,  0.5       ],\n",
              "       [ 0.        , -1.275     , -0.99375   ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        , -7.5       ],\n",
              "       [ 0.9375    ,  0.        , -5.        , -5.        ],\n",
              "       [-7.5       , -1.1625    ,  0.        , -4.25      ],\n",
              "       [11.46150938, -2.023125  , -7.5       , -5.        ],\n",
              "       [-2.75      , -1.        ,  0.        , -0.775     ],\n",
              "       [ 0.19453125, -1.275     , -5.        , -5.        ],\n",
              "       [-5.        ,  0.        ,  0.        , -5.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [-7.5       , -7.275     ,  0.        ,  0.75      ],\n",
              "       [ 0.        , -5.        , -1.1625    ,  0.725     ],\n",
              "       [ 0.        , -4.775     ,  0.        ,  0.975     ],\n",
              "       [ 1.38125   , -8.4125    ,  0.        ,  0.725     ],\n",
              "       [ 0.75      , -7.1625    , -0.775     ,  0.        ],\n",
              "       [ 0.        , -4.56125   ,  0.        ,  0.5       ],\n",
              "       [-4.6625    ,  0.        ,  0.        ,  0.5       ],\n",
              "       [-7.5       , -4.775     , -0.775     ,  0.75      ],\n",
              "       [-2.4375    , -8.925     , -0.775     , -1.05      ],\n",
              "       [ 0.        ,  0.        ,  0.75      , -0.6625    ],\n",
              "       [ 1.1       ,  0.        ,  0.        , -5.        ],\n",
              "       [ 0.        , -5.        ,  0.5       ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.75      ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        , -5.        ],\n",
              "       [ 0.975     , -4.775     ,  0.        , -4.775     ],\n",
              "       [ 1.07625   , -4.775     , -4.775     , -7.06125   ],\n",
              "       [ 0.975     , -7.5       , -4.775     , -8.75      ],\n",
              "       [ 0.975     , -5.        , -7.5       , -7.5       ],\n",
              "       [ 0.        ,  0.        , -6.94875   , -5.        ],\n",
              "       [ 0.975     ,  0.        , -5.        , -7.275     ],\n",
              "       [ 0.875     , -4.775     , -8.31125   ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [-5.        , -9.375     ,  0.        , -7.5       ]])"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "episode = 1\n",
        "teste = np.load(f'q_table.npy')\n",
        "teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1-ftTC3iarM"
      },
      "outputs": [],
      "source": [
        "agent.q_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tEKxWxA1Z-V"
      },
      "outputs": [],
      "source": [
        "  def learn(self, state, action, reward, next_state, done):\n",
        "    # state_unidim = state[0] * self.grid_size + state[1]\n",
        "    # next_state_unidim = next_state[0] * self.grid_size + next_state[1]\n",
        "    old_value = self.q_table[state, action]\n",
        "    next_max = np.max(self.q_table[next_state])\n",
        "\n",
        "    # Fórmula de atualização Q-learning\n",
        "    new_value = (1 - self.learning_rate) * old_value + self.learning_rate * (reward + self.discount_factor * next_max)\n",
        "    self.q_table[next_state,action] = new_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srVdY7xS2RxD"
      },
      "outputs": [],
      "source": [
        "agent.q_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vkdyLFzr9mB"
      },
      "outputs": [],
      "source": [
        "next_state_index = next_state[0] * int(grid_size) + next_state[1]\n",
        "next_state_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BR-xg3CK5a0x"
      },
      "outputs": [],
      "source": [
        "\n",
        "agent.learn(15,2,-1,16,0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfaS3SC-j466"
      },
      "source": [
        "2. Definir o Agente de RL\n",
        "* Função __init__:\n",
        "> Inicializa o agente, define a política inicial (pode ser aleatória), parâmetros de aprendizado, e qualquer estrutura de dados necessária (como tabelas Q ou modelos de rede neural).\n",
        "\n",
        "* Funções para Política de Ação:\n",
        "> Funções para decidir ações, como choose_action, baseadas na política atual.\n",
        "\n",
        "* Função de Aprendizado:\n",
        ">learn ou update_policy para ajustar a política com base nas experiências (atualizando a tabela Q, parâmetros da rede neural, etc.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tc_08r1TkD0j"
      },
      "source": [
        "\n",
        "4. Avaliação e Ajustes\n",
        "Avaliar o Desempenho:\n",
        "\n",
        "Periodicamente testar a política do agente em uma configuração sem aprendizado para avaliar o desempenho.\n",
        "Ajustes de Hiperparâmetros:\n",
        "\n",
        "Ajustar hiperparâmetros do agente com base no desempenho.\n",
        "5. Salvar e Carregar Modelos (Opcional)\n",
        "Funções para Salvar e Carregar:\n",
        "Funções para salvar o estado do agente (como uma tabela Q ou pesos de rede neural) e carregar para futuras execuções ou análises.\n",
        "Ordem Recomendada para a Criação:\n",
        "Ambiente: Defina primeiro o ambiente, pois ele dita as regras, os estados e as ações disponíveis.\n",
        "Agente: Com o ambiente definido, você pode construir o agente que interagirá com este ambiente.\n",
        "Loop de Treinamento: Após ter o ambiente e o agente, crie o loop principal que controla o processo de treinamento.\n",
        "Avaliação e Ajustes: Implemente a avaliação do desempenho e ajuste conforme necessário.\n",
        "Funções de Salvamento: Por fim, adicione a capacidade de salvar e carregar o estado do agente.\n",
        "Lembre-se, cada projeto de RL pode ter suas especificidades, então essa estrutura pode precisar ser adaptada de acordo com as necessidades do seu projeto específico.]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJZZX9XmP+KKKNzpshabIU",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}